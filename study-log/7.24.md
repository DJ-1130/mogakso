# 📘 2025년 7월 25일 스터디 요약
## 주제: 강화학습 (MDP와 Planning / Model-Free Policy Evaluation)

### 🔹 주요 개념 학습

#### ✅ MDP (Markov Decision Process)
- **정의**: (S, A, P, R, γ)
  - S: 상태 집합
  - A: 행동 집합
  - P: 상태 전이 확률 (p(s′|s, a))
  - R: 보상 함수
  - γ: 할인율 (0 ≤ γ ≤ 1)
- **목표**: 총 기대 보상 극대화

#### ✅ MRP (Markov Reward Process)
- MDP에서 action이 빠진 구조
- MRP 값 계산:
  - 벨만 방정식: V(s) = R(s) + γ Σ P(s′|s) V(s′)

#### ✅ 정책 (Policy)
- π(s): 상태 s에서의 행동 선택 전략
- 확률적 정책(π(a|s)) 또는 결정론적 정책

---

### 🧠 알고리즘 학습

#### 🔸 Policy Evaluation (정책 가치 평가)
- 주어진 정책 π에 대해 Vπ(s) 계산
- 방법:
  1. 동적 계획법 (DP)
  2. Monte Carlo 방법
  3. Temporal Difference (TD)

#### 🔸 Policy Iteration
1. 초기 정책 π₀ 설정
2. π 평가 → Vπ 계산
3. π 개선: π(s) ← argmaxₐ Qπ(s, a)
4. 수렴할 때까지 반복

#### 🔸 Value Iteration
- Optimal Bellman Equation 기반 반복 계산
- V(s) ← maxₐ [R(s, a) + γ Σ P(s′|s, a) V(s′)]

---

### 🎲 Model-Free Policy Evaluation

#### 📌 Monte Carlo (MC)
- 에피소드 단위 학습
- **First-Visit MC**: 상태가 처음 등장한 시점의 return만 사용
- **Every-Visit MC**: 등장한 모든 시점의 return 사용
- **특징**: 
  - 높은 분산
  - 상태 방문이 끝난 후 업데이트 가능 (에피소드 완료 필요)

#### 📌 Temporal Difference (TD)
- 한 step 마다 업데이트 (bootstrap)
- TD(0): V(sₜ) ← V(sₜ) + α [rₜ + γV(sₜ₊₁) − V(sₜ)]
- MC와 달리 에피소드가 끝나지 않아도 학습 가능

---

### 📊 알고리즘 비교 요약

| 항목                    | MC             | TD(0)          | DP (모델 기반)  |
|-----------------------|----------------|----------------|----------------|
| 모델 필요 여부         | ❌ 없음        | ❌ 없음        | ✅ 있음        |
| 에피소드 종료 필요     | ✅ 필요        | ❌ 불필요       | ❌ 불필요       |
| 학습 방식              | 샘플 전체      | 한 step        | 완전 모델 기반 |
| 수렴 특성              | 무편향 (일반적) | 바이어스 있음 (하지만 낮은 분산) | 정확함     |
| 부트스트랩 사용        | ❌             | ✅             | ✅             |

---

### 🧩 기타 핵심 내용

- **Return Gₜ**: Gₜ = rₜ + γrₜ₊₁ + γ²rₜ₊₂ + ...
- **Bias / Variance / MSE** 개념 이해
- **Certainty Equivalence (CE)**: 모델을 추정해 DP처럼 사용 (높은 정확도, 낮은 계산 효율)

---



